\documentclass[10pt]{article}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}

% \newcommand{\Xbeta}{ X_i \theta}
\newcommand{\xbeta}{ x_i \beta}
\newcommand{\xtheta}{ x_i \theta}
% \newcommand{\xbetaij}{ x_{ij}^T \theta}
\newcommand{\sgamma}{s_{ij}^T\gamma_i}

\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}


\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{nicefrac}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}


\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage{mhequ}
\newcommand{\be}{\begin{equation}\begin{aligned}}
\newcommand{\ee}{\end{aligned}\end{equation}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Binom}{Binomial}
\DeclareMathOperator{\No}{No}
\DeclareMathOperator{\PG}{PG}
\DeclareMathOperator{\IG}{Inverse-Gamma}
\DeclareMathOperator{\Ga}{Gamma}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\U}{Uniform}
\DeclareMathOperator{\Poi}{Poisson}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\newcommand{\KL}[2]{\textnormal{KL}\left(#1 \parallel #2\right)}

\DeclareMathOperator{\bigO}{\mc O}



\thispagestyle{empty}
\baselineskip=28pt

\title{\textbf{Extrinsic Prior \\for Simple and Efficient Bayesian Modeling \\Under Constraint}}
%\author{Leo Duan, David Dunson}
\date{}
\begin{document}

\maketitle

\section{Introduction}

Literature review:
\begin{itemize}
\item History: Hoffman 1993, Gelfand, Smith and Lee 1992, Danaher et al 2012 -- limited to linear constraint
\item Manifold literature: Gibbs/ rejection sampling -- slow mixing; HMC geodesics method -- difficult and computationally intensive
\item Lagrange/ KKT multiplier, Constraint Bayesian optimization -- point estimate only, no uncertainty
\end{itemize}

Summary of proposed method:
Replacing the inequality and equality constraints with strongly informative prior, allowing carrying out efficient Monte Carlo in a convenient space (e.g. Euchledian space). Obtain posterior,  then use projection to correct the imperfection if needed.

Strength: Large support near the constraint space. Simple and efficient computation. Extremely easy to play, encouraging the community to plug-and-play more constraints in the model.


\section{Method}



The task under consideration involves a $p$-dimensional parameter $\theta$ in a constrained space $\mc D$. Without loss of generality, the space $\mc D$ is assumed to be embedded in another space $\mc R$ (e.g. Euchledean space $\mathbb R^p$) via $m$ equalities and $l$ inequalities, $\mc D = \{ \theta \in \mc R: E_k(\theta)=0$ \text{ for } $k=1,\ldots,m, \quad G_{k'}(\theta)\le 0  \text{ for } k'=1,\ldots,l \}$, where $E_k(.)$ and $G_{k'}(.)$ are functions that map from $\mc R$ to real line $\mathbb R$. These functions are differentiable with respect to $\theta$ and not necessarily linear.

Throughout this section, we use one example to illustrate the embedding and the method. Consider an {\it ordered} $(d-1)$-simplex. The parameter is a $d$-dimensional probability vector $\theta=\{p_1,\ldots,p_d\}$ with $p_1\ge p_2 \ge \ldots \ge p_d$. Its space $\mc D$ is embedded in $[0,1]^d$ via $d-1$ inequality constraints $p_{i+1}-p_i \le 0$ for $i=1,\ldots, d-1$ and one equality constraint $\sum_{i=1}^{d} p_i-1 = 0$. Alternatively, one can view the space $\mc D$ as embedded in a broader space $\mathbb R^d$, via additional $d$ identity inequalities $p_i \ge 0$ for $i=1,\ldots,d$; however this is not necessary since in general, constraints via identity functions as such are trivial to handle. Therefore, from now on we assume that all chosen space $\mc R$ has already accommodated the simple identity constraints, using space truncation.

We hope to obtain statistical inference on $\theta$ in this constrained space $\mc D$. Letting $L(\theta;y)$ be the likelihood and $\pi_0(\theta \mid \theta \in \mc D)$ be the prior, given observed data $y$, we are interested in the posterior distribution: 

\be
\pi(\theta \mid y, \theta \in \mc D) = \frac{ L(\theta;y)\pi_0(\theta \mid \theta\in \mc D)}{\int_{\mc D} L(\theta;y)\pi_0(\theta \mid \theta\in \mc D) d\theta },
\ee
where the prior $\pi_0(\theta \mid \theta \in \mc D) = \frac {\pi_0(\theta)}{\int_{\mc D} \pi_0(\theta) d\theta}$ with $\pi_0(\theta)$ defined in $\mc R$. Due to the space integrated over, $\int_{\mc D} \pi_0(\theta) d\theta$ often lacks closed-form; but since it is a constant, one commonly utilize:

\be
\pi(\theta \mid y, \theta \in \mc D) \propto  L(\theta;y)\pi_0 (\theta), \quad\theta\in \mc D
\ee

To satisfy $\theta \in \mc D$, it often demands substantial efforts. One costly strategy is to first propose $\theta\in \mc R$, then reject those violating any of the constraints (i.e. $\theta \not\in \mc D$) (CITES Gelfand et al 1992). Alternatively, one relies on a skillful re-parameterization of $\theta$ to meet the constraint implicitly. For example, in manifold modeling, one often switches to the coordinate system instead of using $\theta$ directly; in unordered simplex modeling, one uses stick-breaking construction instead to meet the fixed $1$-norm constraint. However, any complication like the ordered simplex example would disrupt the solution, making the estimation substantially more difficult.

We now propose a different strategy by replacing the constringent embedding with a set of strongly informative priors. Specifically, instead of modeling $\theta \in \mc D$, we relax its space to an emcompassing and tight neighborhood in $\mc R$. This is achieved through adding $(m+l)$ kernel functions $K_.(.)$, leading to posterior: 


\be
\label{extrinsic_prior}
\pi(\theta \mid y) \propto L(\theta;y)\pi_0(\theta) \cdot \prod_{k=1}^{m} K_{1,k}\Big( E_k(\theta) \Big) \cdot \prod_{k'=1}^{l} K_{2,k'}\Big( \big( G_{k'}(\theta) \big)_+ \Big), \quad \theta \in \mc R
\ee
where $(x)_+ = x$ if $x>0$, $0$ if $x\le 0$. All the kernel functions $K_{.}(.)$ satisfy $K_{.}(0)=1$, so that when $\theta \in \mc D$ exactly (recall $\mc D \subseteq \mc R$), the posterior density $\pi(\theta\mid y)$ is the same as the strict embedding case, except for a constant proportional difference. And $K_.(x)$ declines repaidly when $x\ne 0$. For example, one simple kernel for this purpose is the Gaussian kernel $K_{i,k}(x) = \exp( -{\lambda_{i,k} x^2})$ with large $\lambda_{ik}$.

The kernels are part of prior densities that handle the constraints via an {\it extrinsic} approach. We therefore refer them as extrinsic priors. The posterior obtained using $\eqref{extrinsic_prior}$ is an approximation to those obtained in a strict embedding approach. One can simply use them for the approximate statistical inference, or use correcting projection to map them back to $\mc D$. In this article, we focus on the latter.


\subsection{Prior Specification}

The tightness of the neighborhood is governed by the hyper-parameters in the extrinsic prior. It has mainly two effects on the posterior estimation: (1) the approximation accuracy for the constraints; (2) the posterior mixing, which is related to the convergence speed and posterior autocorrelation of the Markov chain. In general, tighter neighborhood leads to slower mixing. Therefore, a balance needs to be struck when choosing the hyper-parameters. 

Let $\mc E_{i,k}(\theta)=c_{i,k}K_{i,k}(x)$ be the normalized extrinsic prior for the $(i,k)$th constraint ($i=1$ equality, $i=2$ inequality), where $c_.=1/\big( \int_{\mc R} K_{.}(x) dx \big)$. To construct a tight neighborhood, we require  $\int_{|x|<\epsilon} \mc E_{i,k}(\theta) = 1 -\eta$ with $\eta>0$ negligibly small. The constant $\epsilon$ is pre-specified and represents the element-wise tolerance for violating each constraint. The amount of violation is reflected in the posterior values of $E_k(\theta)$ and $\big(G_k(\theta)\big)_+$. For example, in the Gaussian kernel, setting $\lambda_. = \frac{1}{ 2(\epsilon/3)^2}$ ensures each error is contained within a radius of $\epsilon$ from $0$ with each marginal probability $0.997$.

\leo{We probably need some regulartiy condition on $\pi_0(\theta)$ and $L(y;\theta)$, originally defined on $\mc D$: when the space is extended to $\mc R$, they should not have a big increase outside of $\mc D$ (e.g. $\pi_0(\theta)\rightarrow\infty$ would be bad).}

So far we have taken an element-wise approach for specifying the kernel $K_{.}$'s. It is possible to specify $K_{.}$'s in a dependent way and contain the approximation error with probability better than $(1-\eta)^{(l+m)}$. However, unless $l+m$ is very large, this is often unnecessary since a correcting projection will be made to erase the approximation error, after the posterior is collected. Therefore, we focus on controlling the error in acceptable range, allowing good mixing and easy projection of $\theta$ back to $\mc D$.

\leo{more work is to be done on assessing the effects on mixing}

\subsection{Posterior Sampling}

\subsection{Correcting Projection}

\subsection{Illustration: Ordered Simplex}

\section{Theory}
\section{Application}
\section{Discussion}




\end{document}
