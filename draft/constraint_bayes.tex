\documentclass[10pt]{article}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}

% \newcommand{\Xbeta}{ X_i \theta}
\newcommand{\xbeta}{ x_i \beta}
\newcommand{\xtheta}{ x_i \theta}
% \newcommand{\xbetaij}{ x_{ij}^T \theta}
\newcommand{\sgamma}{s_{ij}^T\gamma_i}

\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}


\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{nicefrac}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}


\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage{mhequ}
\newcommand{\be}{\begin{equation}\begin{aligned}}
\newcommand{\ee}{\end{aligned}\end{equation}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Binom}{Binomial}
\DeclareMathOperator{\No}{No}
\DeclareMathOperator{\PG}{PG}
\DeclareMathOperator{\IG}{Inverse-Gamma}
\DeclareMathOperator{\Ga}{Gamma}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\U}{Uniform}
\DeclareMathOperator{\Poi}{Poisson}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\newcommand{\KL}[2]{\textnormal{KL}\left(#1 \parallel #2\right)}

\DeclareMathOperator{\bigO}{\mc O}



\thispagestyle{empty}
\baselineskip=28pt

\title{\textbf{Extrinsic Prior for Simple and Efficient\\ Bayesian Modeling with Parameter Constraints}}
%\author{Leo Duan, David Dunson}
\date{}
\begin{document}

\maketitle


{\bf Abstract:} Models
\vskip 12pt

%\baselineskip=12pt
%\par\vfill\noindent
{\noindent  KEY WORDS:  Bayesian probit; Bayesian logit; Big $n$; Data Augmentation; Maximal Correlation; Polya-Gamma.}
%\par\medskip\noindent
%\clearpage\pagebreak\newpage
\pagenumbering{arabic}

\section{Introduction}

Constraints are very common in statistical modeling. In applied domain, modeling assumptions often require some constraint. For example, in functional data analysis related to degenerative disease, it is common to assume the curve need to satisfy certain shape constraint such as monotonicty \citep{lin2014monogp}. In stastical optimization, constraints such as orthonormality are also routinely used to ensure identifiability of the model \citep{uschmajew2010well}. In manifold modeling, a large class of manifolds can be viewed as sub-manifolds of a more conventional space (e.g. Euchlean space), embedded via different constraints.

These constraints can cause substantial modeling difficulty. When constraints are applied on the data, they could generate an intractable integral with the parameter in the likelihood, leading to a ``doubly intractable'' problem. Several successful solutions have been proposed to address this issue \citep{murray2012mcmc,rao2016data}.
On the other hand, there is a clear lack of general and simple solution, when the constraints are applied on the parameters. In frequentist optimization literature, the use of Lagrange and Karush-Kuhn-Tucker multipliers provides a means to obtain point estimate under the equality and inequality constraints \citep{boyd2004convex}. But due to the space constraint, the standard asympotic approximation for variance estimation usually do not hold. Therefore, a Bayesian approach would be more approriate to quantify the uncertainty. Ideally, one would assign prior on a constraint support, then utilize standard toolbox such as Markov chain Monte carlo (MCMC) to obtain posterior sample on this space. However, this turns out to be very challenging.

To assign prior on the constraint space, the available families of distribution are often quite limited. For example, for othornormal matrices on the Stiefel manifold, the matrix von Mises-Fisher distribution \citep{khatri1977mises} is one of the only few choices. For regression under linear inequality constraints, only until recently a tractable prior is proposed for the polyhedral region set by the inequailities \citep{danaher2012minkowski}. Alternatively, \cite{gelfand1992bayesian} proposed a truncation strategy by first considering common unrestricted distribution, then assigning zero support outside the constraint region. Accordingly, the posterior estimation proceeds in first generating unrestricted proposals using Gibbs sampling, then only accepting those inside the constraint space. Although this approach allows using a more general class of prior distribution, the drawback is that the unrestricted proposal can have significant mass outside the constraint region, resulting in a high rejection rate.

To meet the constraints, efficient computation is elusive and often demands substantial efforts to develop. And often it can be disrupted by slight complication such as hierarchical structure or additional constraint. For example, stick-breaking parameterization is commonly used in probability simplex modeling, in order to circumvent the constraint of vertices summing to $1$. However, its computational efficency can be broken by additional structure constraint, such as the ordering of the simplex, which is useful in reducing the label switching problem in mixture modeling \citep{diebolt1994estimation}. As another example, in multiway tensor factorization, orthonormality is useful to induce good posterior mixing in estimating factor matrices. This largely relies on the sampler of Bingham-von Mises-Fisher distribution \citep{hoff2016equivariant}. However, when there is symmetry in the slices of tensor (commonly in population of undirected networks), at least two factors would be the same. This disrupts the closed form of the posterior, demanding new rejection sampling algorithm to be developed. The Bayesian manifold modeling also faces the same quadmire. Hamiltonian Monte Carlo accomodating the geometric structure of the manifold have been developed \citep{girolami2011riemann,byrne2013geodesic}, but the computation is intensive and mixing is suboptimal. These challenges prohibit a good utilization of constraints in statistics.

We propose to solve this problem by viewing the constrigent constraints as a limiting case of a strongly informative prior, referred as extrisic prior. We then relax the effective support of the prior to the neighbor of the constraint space, allowing approximate posterior to be collected efficiently via Hamiltonian Monte Carlo directly in Euchledean space. The imperfection of approximation can be corrected by an efficient reconstruction of an exact Markov chain after projection. The proposed approach is simple to implement and can be automatically carried out via software such as STAN. Theoretic studies are conducted and substantial improvement is shown in simulations and data application.




\section{Method}

 unless the likelihood involve 

 intractable problem. 


can usually be considered as assigning a prior in a 

The task under consideration involves a $p$-dimensional parameter $\theta$ in a constrained space $\mc D$. Without loss of generality, the space $\mc D$ is assumed to be embedded in another space $\mc R$ (e.g. Euchledean space $\mathbb R^p$) via $m$ equalities and $l$ inequalities, $\mc D = \{ \theta \in \mc R: E_k(\theta)=0$ \text{ for } $k=1,\ldots,m, \quad G_{k'}(\theta)\le 0  \text{ for } k'=1,\ldots,l \}$, where $E_k(.)$ and $G_{k'}(.)$ are functions that map from $\mc R$ to real line $\mathbb R$. These functions are differentiable with respect to $\theta$ and not necessarily linear.

Throughout this section, we use one example to illustrate the embedding and the method. Consider an {\it ordered} $(d-1)$-simplex. The parameter is a $d$-dimensional probability vector $\theta=\{p_1,\ldots,p_d\}$ with $p_1\ge p_2 \ge \ldots \ge p_d$. Its space $\mc D$ is embedded in $[0,1]^d$ via $d-1$ inequality constraints $p_{i+1}-p_i \le 0$ for $i=1,\ldots, d-1$ and one equality constraint $\sum_{i=1}^{d} p_i-1 = 0$. Alternatively, one can view the space $\mc D$ as embedded in a broader space $\mathbb R^d$, via additional $d$ identity inequalities $p_i \ge 0$ for $i=1,\ldots,d$; however this is not necessary since in general, constraints via identity functions as such are trivial to handle. Therefore, from now on we assume that all chosen space $\mc R$ has already accommodated the simple identity constraints, using space truncation.

We hope to obtain statistical inference on $\theta$ in this constrained space $\mc D$. Letting $L(\theta;y)$ be the likelihood and $\pi_0(\theta \mid \theta \in \mc D)$ be the prior, given observed data $y$, we are interested in the posterior distribution: 

\be
\pi(\theta \mid y, \theta \in \mc D) = \frac{ L(\theta;y)\pi_0(\theta \mid \theta\in \mc D)}{\int_{\mc D} L(\theta;y)\pi_0(\theta \mid \theta\in \mc D) d\theta },
\ee
where the prior $\pi_0(\theta \mid \theta \in \mc D) = \frac {\pi_0(\theta)}{\int_{\mc D} \pi_0(\theta) d\theta}$ with $\pi_0(\theta)$ defined in $\mc R$. Due to the space integrated over, $\int_{\mc D} \pi_0(\theta) d\theta$ often lacks closed-form; but since it is a constant, one commonly utilize:

\be
\pi(\theta \mid y, \theta \in \mc D) \propto  L(\theta;y)\pi_0 (\theta), \quad\theta\in \mc D
\ee

To satisfy $\theta \in \mc D$, it often demands substantial efforts. One costly strategy is to first propose $\theta\in \mc R$, then reject those violating any of the constraints (i.e. $\theta \not\in \mc D$) (CITES Gelfand et al 1992). Alternatively, one relies on a skillful re-parameterization of $\theta$ to meet the constraint implicitly. For example, in manifold modeling, one often switches to the coordinate system instead of using $\theta$ directly; in unordered simplex modeling, one uses stick-breaking construction instead to meet the fixed $1$-norm constraint. However, any complication like the ordered simplex example would disrupt the solution, making the estimation substantially more difficult.

We now propose a different strategy by replacing the constringent embedding with a set of strongly informative priors. Specifically, instead of modeling $\theta \in \mc D$, we relax its space to an emcompassing and tight neighborhood in $\mc R$. This is achieved through adding $(m+l)$ kernel functions $K_.(.)$, leading to posterior: 


\be
\label{extrinsic_prior}
\pi(\theta \mid y) \propto L(\theta;y)\pi_0(\theta) \cdot \prod_{k=1}^{m} K_{1,k}\Big( E_k(\theta) \Big) \cdot \prod_{k'=1}^{l} K_{2,k'}\Big( \big( G_{k'}(\theta) \big)_+ \Big), \quad \theta \in \mc R
\ee
where $(x)_+ = x$ if $x>0$, $0$ if $x\le 0$. All the kernel functions $K_{.}(.)$ satisfy $K_{.}(0)=1$, so that when $\theta \in \mc D$ exactly (recall $\mc D \subseteq \mc R$), the posterior density $\pi(\theta\mid y)$ is the same as the strict embedding case, except for a constant proportional difference. And $K_.(x)$ declines repaidly when $x\ne 0$. For example, one simple kernel for this purpose is the Gaussian kernel $K_{i,k}(x) = \exp( -{\lambda_{i,k} x^2})$ with large $\lambda_{ik}$.

The kernels are part of prior densities that handle the constraints via an {\it extrinsic} approach. We therefore refer them as extrinsic priors. The posterior obtained using $\eqref{extrinsic_prior}$ is an approximation to those obtained in a strict embedding approach. One can simply use them for the approximate statistical inference, or use correcting projection to map them back to $\mc D$. In this article, we focus on the latter.


\subsection{Prior Specification}

The tightness of the neighborhood is governed by the hyper-parameters in the extrinsic prior. It has mainly two effects on the posterior estimation: (1) the approximation accuracy for the constraints; (2) the posterior mixing, which is related to the convergence speed and posterior autocorrelation of the Markov chain. In general, tighter neighborhood leads to slower mixing. Therefore, a balance needs to be struck when choosing the hyper-parameters. 

Let $\mc E_{i,k}(\theta)=c_{i,k}K_{i,k}(x)$ be the normalized extrinsic prior for the $(i,k)$th constraint ($i=1$ equality, $i=2$ inequality), where $c_.=1/\big( \int_{\mc R} K_{.}(x) dx \big)$. To construct a tight neighborhood, we require  $\int_{|x|<\epsilon} \mc E_{i,k}(\theta) = 1 -\eta$ with $\eta>0$ negligibly small. The constant $\epsilon$ is pre-specified and represents the element-wise tolerance for violating each constraint. The amount of violation is reflected in the posterior values of $E_k(\theta)$ and $\big(G_k(\theta)\big)_+$. For example, in the Gaussian kernel, setting $\lambda_. = \frac{1}{ 2(\epsilon/3)^2}$ ensures each error is contained within a radius of $\epsilon$ from $0$ with each marginal probability $0.997$.

\leo{We probably need some regulartiy condition on $\pi_0(\theta)$ and $L(y;\theta)$, originally defined on $\mc D$: when the space is extended to $\mc R$, they should not have a big increase outside of $\mc D$ (e.g. $\pi_0(\theta)\rightarrow\infty$ would be bad).}

So far we have taken an element-wise approach for specifying the kernel $K_{.}$'s. It is possible to specify $K_{.}$'s in a dependent way and contain the approximation error with probability better than $(1-\eta)^{(l+m)}$. However, unless $l+m$ is very large, this is often unnecessary since a correcting projection will be made to erase the approximation error, after the posterior is collected. Therefore, we focus on controlling the error in acceptable range, allowing good mixing and easy projection of $\theta$ back to $\mc D$.

\leo{more work is to be done on assessing the effects on mixing}

{\bf Example 1: Ordered Simplex}

{\bf Example 2: Monotone Spline} 

{\bf Example 3: Orthonormal Gaussian Processes} 

\subsection{Posterior Sampling}

\subsection{Correcting Projection}

\subsection{Illustration: Ordered Simplex}

\section{Theory}
\section{Application}
\section{Discussion}



\bibliography{reference}
\bibliographystyle{chicago}

\end{document}
