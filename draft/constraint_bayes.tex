\documentclass[10pt]{article}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}

% \newcommand{\Xbeta}{ X_i \theta}
\newcommand{\xbeta}{ x_i \beta}
\newcommand{\xtheta}{ x_i \theta}
% \newcommand{\xbetaij}{ x_{ij}^T \theta}
\newcommand{\sgamma}{s_{ij}^T\gamma_i}

\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}
\usepackage{bbm}

\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{nicefrac}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}


\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage{mhequ}
\newcommand{\be}{\begin{equation}\begin{aligned}}
\newcommand{\ee}{\end{aligned}\end{equation}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Binom}{Binomial}
\DeclareMathOperator{\No}{No}
\DeclareMathOperator{\PG}{PG}
\DeclareMathOperator{\IG}{Inverse-Gamma}
\DeclareMathOperator{\Ga}{Gamma}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\U}{Uniform}
\DeclareMathOperator{\Poi}{Poisson}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\newcommand{\KL}[2]{\textnormal{KL}\left(#1 \parallel #2\right)}

\DeclareMathOperator{\bigO}{\mc O}



\thispagestyle{empty}
\baselineskip=28pt

\title{\textbf{Extrinsic Prior for Simple and Efficient\\ Bayesian Modeling with Parameter Constraints}}
%\author{Leo Duan, David Dunson}
\date{}
\begin{document}

\maketitle


{\bf Abstract:} Parameter constraints are commonly seen in statistical models, such as linear inequality, simplex constraint, parameter ordering, monotonicity, orthogonality, etc. Bayesian approach is useful for uncerntainty quantification in the constrained space. Although customized solutions have been made for different constraints, it is difficult to carry out estimation in general cases, especially when the posterior lacks closed-form or the model is heavily constrained. In this paper, we propose a simple and general solution by first replacing constraints with strongly informative prior. Through this {\it extrinsic} prior, the parameter support is relaxed to a less restrictive space, where conventional tools such as Hamiltonian Monte Carlo can be exploited to efficiently obtain approximate posterior.  These posteriors can then be projected back to the constrained space for exact solution. This approach is very efficient and applicable to a wide range of problems including ones with equality and inequality constraints. Since priors are no longer limited to ones with closed-form posterior, more distribution families can be chosen for the constrained parameters. Multiple constraints can be freely adpoted for desired property such as model identifiability. Theory is developed and novel statistical applications under constraints are demonstrated.
\vskip 12pt

%\baselineskip=12pt
%\par\vfill\noindent
{\noindent  KEY WORDS:  Constraint relaxation; Space embedding; Monotone Dirichlet; Orthogonal Gaussian processes; Posterior mixing; Projected Markov chain}
%\par\medskip\noindent
%\clearpage\pagebreak\newpage
\pagenumbering{arabic}

\section{Introduction}

Constraints are commonly assumed in modern statistical models. For example, functional data anaylsis often imposes constraint on shape, such as monotonicity or convexity on curves \citep{kelly1990monotone}; matrix and tensor decomposition utilize orthonormality to remove scaling and rotation problem in indentifiability \citep{uschmajew2010well}; many manifolds such as simplex can be considered as sub-manifolds of a Euclidean space embedded via  constraints.

When data are in constrained space, parameters can enter the likelihood via an integral without closed-form, leading to ``doubly intractable'' problem. Various successful solutions have been proposed for this issue \citep{murray2012mcmc,rao2016data}. When parameters are constrained, challenges often arise in estimation. Frequentist optimization literature often relies on Lagrange and Karush-Kuhn-Tucker multipliers for point estimate under equality and inequality constraints \citep{boyd2004convex}. However, uncertainty quantification is difficult since conventional asympotic result on variance estimation often no longer hold in constrained space. In this regard, Bayesian approach is more approriate.

There have been a variety of customized solutions developed for specific constraints. One strategy involves using constrained prior with posterior that can be conveniently sampled. For example, for modeling othornormal matrices on the Stiefel manifold, Bingham-von-Mises-Fisher distribution \citep{khatri1977mises,hoff2009simulation} is a parametric family used in matrix and tensor decomposition. \cite{lin2016bayesstiefel} extends the flexibility of matrix von-Mises-Fisher distribution via non-parametric approach. Another strategy is to bypass the constraint via re-parameterization. The famous example is the stick-breaking construction for Dirichlet distribution and process. The re-parameterization essentially utilizes the coordinate system of the simplex, and circumvents the norm constraint on the probability vertices. As these methods directly satisfy the constraint requirement, we refer them as the intrinsic approaches. Despite the success of intrinsic approaches,  posteriors can quickly become very involved in Gibbs sampling, leading to inefficient posterior mixing. The closed-form solution also often breaks under slightly more advanced model or complicated data. For example, in modeling population of undirected networks, the symmetry in each network disrupts the closed form of posterior in orthogonal tensor decomposition proposed by \cite{hoff2016equivariant}, demanding new rejection sampling algorithm to be developed. As another example, additional strucutre (such as ordering) on the probability simplex often disrupts the simple form of stick-breaking posterior.

These drawbacks have motivated the development of extrinsic approaches. The key idea is to first sample the proposal freely in a conventional space (such as Euclidean space), then transform it back to the constrained space. One early work can be traced back to \cite{gelfand1992bayesian}, who used Gibbs sampling to first generate proposal in unrestricted region, then only accepting those falling inside the constraint space. One critical issue is that unrestricted proposal can have significant mass outside the constraint region, resulting in a high rejection rate. Replacing rejection sampling, \cite{lin2014monogp} and \cite{lin2016extrinsic} utilize projection to map the unconstrained posterior into the constrained space and obtain monotonicity and manifold-valued regression. \cite{neal2011mcmc} suggested using large penalty to create a energy wall to guide the Hamiltonian dynamics involving simple space truncation, and accept proposal only when it is inside the truncated space. \cite{pakman2014exact} applied similar idea in making the generation of truncated multivariate normal more efficient. These specialized cases work well, but such case are often rare and there is a clear lack of general and simple approach. 

In this paper, we propose a general extrinsic approach, by parameterizing constraints as a limiting case of strongly informative prior. We refer them as extrisic priors. We then relax the effective support of the prior to a neighborhood of constraint space, obtaining posterior via efficient tools such as conventional Hamiltonian Monte Carlo (HMC). These posteriors are approximate to the canonical formulation, with approximation error bounded during the prior specification. The imperfection of approximation can be corrected with a simple projection and a Metropolis-Hastings step with high acceptance probability, leading to a Markov chain corresponding to the exact formulation. Compared to other manifold based methods such as Riemannian and geodesic HMC \citep{girolami2011riemann,byrne2013geodesic}, our approach is efficient in computation and simple to implement via highly automatic software like STAN. The simplicity enables a larger spectrum of prior to be chosen and more free adaoption of constraints in modeling. Theoretic studies are conducted and original models are shown in simulations and data application.

\section{Method}

We consider a parameter $\theta$ in a constrained space $\mc D$. The space $\mc D$ can be high- or infinite-dimensional. Letting this space be equiped with a $\sigma$-field $\mathscr D$, the standard Bayesian approach assigns a prior for $\theta$ in $\mc D$, based on a density $\pi_{0,\mc D}(\theta)$ in a separable space $(\mc D, \mathscr D)$. In intrinsic approaches, priors are chosen for computational conveniences so that the posterior can be easily sampled strictly inside $\mc D$. Clearly, the choices of priors and constraints one can impose are very limited. Instead, we consider extrinsic approaches by estimating $\theta$ in the larger space $\mc R$ where $\mc D\in \mc R$. We first provide a probabilistic justification.


Assuming $\pi_{0,\mc D}(\theta)$ is proper $\int_{\mc D} \pi_{0,\mc D}(\theta) d\theta <\infty$, then this prior can be viewed as a conditional density, based on another density $\pi_{0,\mc R}(\theta)$ in $(\mc R, \mathscr C)$ with $\mathscr C$ as the $\sigma$-field of $\mc R$:

\begin{equation}
\begin{aligned}
\pi_{0,\mc D}(\theta)= \pi_{0,\mc R}(\theta \mid \theta \in \mc D) = \frac{ \pi_{0,\mc R}(\theta) \mathbbm{1}_{\theta\in \mc D} }{ \int_{\mc D}  \pi_{0,\mc R}(\theta)d\theta }.
\end{aligned}
\end{equation}
where $\mathbbm{1}_{\theta \in \mc D}=1$ when $\theta \in \mc D$, $0$ otherwise. Note  as long as $\pi_{0,\mc R}(\theta) \mathbbm{1}_{\theta\in \mc D}$ is proper, $\pi_{0,\mc R}(\theta)$ can be improper. Letting $L(\theta;y)$ be the likelihood function and $y$ be the observed data, the posterior can be obtained via:

\begin{equation}
\begin{aligned}
\label{exact_posterior}
\pi(\theta \mid y, \theta \in \mc D) = \frac{ L(\theta;y)\pi_{0,\mc D}(\theta)}{\int_{\mc D} L(\theta;y)\pi_{0,\mc D}(\theta) d\theta } = \frac{ L(\theta;y)\pi_{0,\mc R}(\theta) \mathbbm{1}_{\theta\in \mc D} }{\int_{\mc D} L(\theta;y)\pi_{0,\mc R}(\theta) d\theta },
\end{aligned}
\end{equation}
where the last equality holds because $\int_{\mc D}  \pi_{0,\mc R}(\theta) d\theta$ is a finite constant.

%Throughout this section, we use one example to illustrate the embedding and the method. Consider an {\it ordered} $(d-1)$-simplex. The parameter is a $d$-dimensional probability vector $\theta=\{p_1,\ldots,p_d\}$ with $p_1\ge p_2 \ge \ldots \ge p_d$. Its space $\mc D$ is embedded in $[0,1]^d$ via $d-1$ inequality constraints $p_{i+1}-p_i \le 0$ for $i=1,\ldots, d-1$ and one equality constraint $\sum_{i=1}^{d} p_i-1 = 0$. Alternatively, one can view the space $\mc D$ as embedded in a broader space $\mathbb R^d$, via additional $d$ identity inequalities $p_i \ge 0$ for $i=1,\ldots,d$; however this is not necessary since in general, constraints via identity functions as such are trivial to handle. Therefore, from now on we assume that all chosen space $\mc R$ has already accommodated the simple identity constraints, using space truncation.


% To satisfy $\theta \in \mc D$, it often demands substantial efforts. One costly strategy is to first propose $\theta\in \mc R$, then reject those violating any of the constraints (i.e. $\theta \not\in \mc D$) (CITES Gelfand et al 1992). Alternatively, one relies on a skillful re-parameterization of $\theta$ to meet the constraint implicitly. For example, in manifold modeling, one often switches to the coordinate system instead of using $\theta$ directly; in unordered simplex modeling, one uses stick-breaking construction instead to meet the fixed $1$-norm constraint. However, any complication like the ordered simplex example would disrupt the solution, making the estimation substantially more difficult.

\subsection{Extrinsic Prior for Constraints}

One obvious extrinsic approach utilizing \eqref{exact_posterior} is to first generate proposal in $\mc R$ based on $L(\theta;y)\pi_{0,\mc R}(\theta)$ (assuming it is proper), then accepting it when it falls in $\mc D$ \citep{gelfand1992bayesian}. However, when the probability $Pr(\theta\in \mc D \mid y) / Pr(\theta\in \mc R \setminus D \mid y) \approx 0$, especially common in equality constraint, this would lead to most of the proposals being rejected.

We propose a different strategy. Instead of ignoring $\mathbbm{1}_{\theta\in \mc D} $ in the first step, we approximate it with an additional strongly informative prior $\mc E(\theta)$. The prior has its support in $\mc R$, but its mass concentrated near $\mc D$. When $\theta \in \mc D$, $\mc E(\theta)$ is constant; when $\theta$ is outside of $D$, $\mc E(\theta)$ quickly drops to $0$. Then one can first obtain approximate posterior based on density proportional to $L(\theta;y)\pi_{0,\mc R}(\theta)\mc E(\theta)$ (the conditions for posterior properity is postponed to the theory section).

In this paper, we focus on $\mc D$ that can be embedded in $\mc R$ via equality and inequality constraints, although other types of constraints can be incorporated similarly. There are $m$ equalities and $l$ inequalities, leading to $\mc D = \{ \theta \in \mc R: E_k(\theta)=0 \text{ for } k=1,\ldots,m, \quad G_{k'}(\theta)\le 0  \text{ for } k'=1,\ldots,l \}$, where $E_k(.)$ and $G_{k'}(.)$ are functions that map from $\mc R$ to real line $\mathbb R$. Then the indicator function is $\mathbbm{1}_{\theta\in \mc D} = \prod_k \mathbbm{1}_{E_k(\theta)=0} \cdot \prod_{k'}\mathbbm{1}_{G_k'(\theta)\le 0}$.


We now replace the indicator functions with $\mc E(\theta)$, represented as a product of $(m+l)$ kernel functions $K_.(.)$, leading to posterior:

\begin{equation}
\begin{aligned}
\label{extrinsic_prior}
\pi_{K}(\theta \mid y) & \propto L(\theta;y)\pi_{0,\mc R}(\theta) \mc E(\theta) \\
&\propto L(\theta;y)\pi_{0,\mc R}(\theta) \cdot \prod_{k=1}^{m} K_{1,k}\Big( | E_k(\theta)| \Big) \cdot \prod_{k'=1}^{l} K_{2,k'}\Big( \big( G_{k'}(\theta) \big)_+ \Big)
\end{aligned}
\end{equation}
where $(x)_+ = x$ if $x>0$, $0$ if $x\le 0$. The posterior $\pi_{K}(\theta \mid y)$ is an approximation to $\pi(\theta \mid y)$ in \eqref{exact_posterior}. We will now refer $\pi_{K}(\theta \mid y)$ as ``extrinsic posterior''. The functions $|E_k(\theta)|\in [0,\infty)$ or $(G_{k'}(\theta))_+ \in [0,\infty)$ represent the amount of relaxation for each constraint, where $0$ represents no relaxation. Each kernel $K_{i,k}$ satisfies $K_{i,k}(0)=1$; the tolerable amount of relaxation is controlled by a hyper-parameter $\lambda_{i,k}$. When $\lambda_{i,k} \rightarrow \infty$, the kernel becomes a point mass at $0$. Therefore, \eqref{exact_posterior} is a limiting case of \eqref{extrinsic_prior}. For example, one simple and useful kernel is the truncated Gaussian $K_{i,k}(x) = \exp( -{\lambda_{i,k} x^2}) \mathbbm{1}_{x<\varepsilon}$.

Instead of taking infinite values for $\lambda$'s, we assign large but finite ones. This gives rise to a continuous relaxation of the sharp boundary of the indicator function. The relaxation allows the posterior $\theta$ to be easily sampled in $\mc R$ under the guidance of the strongly informative prior $\mc E(\theta)$. For example, one can carry out conventional HMC for constrained parameters directly in Euchledean space. At the same time, since posteriors are generated in a tight neighborhood of $\mc D$, they can be easily projected back to $\mc D$ as to produce exact posterior in $\mc D$.

\subsection{Control of Constraint Relaxation}

In the extrinsic posterior \eqref{extrinsic_prior}, when $\theta \in \mc D$, the density is the same as \eqref{exact_posterior}, up to a constant difference. However, since we induce positive support in $\mc R \setminus \mc D$, it is important to control the approximate posterior close to the space $\mc D$. This can be measured by the posterior distribution of the constraint relaxation $|E_k(\theta)|$ and $(G_{k'}(\theta))_+$.

We control the amount of constraint relaxation via a bounded prior support near $0$ for each kernel.  That is $\int_{x<\varepsilon} \mc C_{i,k}(x)  dx= 1$, with  $\mc C_{i,k}(x) = K_{i,k}(x) / \int_{\mc R}K_{i,k}(x) dx$. The pre-specified constant $\varepsilon$ represents the element-wise tolerance for violating each constraint. The bounded prior support allows us to theoretically control the posterior approximation error. As $\mc E(\theta) \propto \prod_{i,k} \mc C_{i,k}(x)$ is the joint extrinsic prior density, since $\pi_K(\theta \mid y) \ll \mc E(\theta)$, the posterior for each constraint relaxation is bounded in $[0,\varepsilon)$ with probability $1$.

In practice, one may wish to utilize a kernel $K^*_{i,k}(x)$, orginally with unbounded support on $[0,\infty)$ for computing conveniences. To adopt them for bounded support in the relaxation $x$, one can first choose $\lambda_{i,k}$ to have $\int_{x<\varepsilon} \mc K^*_{i,k}(x)/ \big( \int_{\mc R} K^*_{.}(x) dx\big ) = 1-\eta$ with $\eta$ small, then apply truncation $K_{i,k}(x)= K^*_{i,k}(x) \mathbbm{1}_{x<\varepsilon}$ to ensure $x<\varepsilon$ almost surely. In most cases, the truncation is only nominal for a theoretic guarantee; in computation it is often not needed. For example, in Gaussian kernel $\exp( -{\lambda x^2})$ setting $\lambda = \frac{1}{ 2(\varepsilon/4)^2}$ ensures the relaxation $x<\varepsilon$ with probability $0.99993$ apriori; for posterior sampling, one can first do an untruncated sampling, then reject those $x<\varepsilon$, which quite rare due to the small prior probability. 


To illustrate the approximation of extrinsic prior and control of constraint relaxation, we consider a simple example of generating a truncated Gaussian distribution $\theta \mid y \sim \No_{(\alpha,\beta)}(0,1)$, with mean $0$ and variance $1$ and truncation $\theta\in (\alpha,\beta)$. The exact and extrinsic posterior densities are:

$$\pi(\theta\mid y)\propto \exp(-\frac{\theta^2}{2}) \mathbbm{1}_{\theta\in(\alpha,\beta)}, \quad \pi_K(\theta\mid y)\propto \exp(-\frac{\theta^2}{2}) K \left ( (\alpha - \theta)_+ \right) K \left ( ( \theta - \beta)_+ \right).$$
with $K(x)= \exp( - \lambda x^2)\mathbbm{1}_{x<4/\sqrt{2\lambda}}$. We set $(\alpha, \beta)=(1,2)$. Figure~\ref{truncated_normal} plots the unnormalized densities under the exact posterior and approximation with different $\lambda$'s. The approximate densities inside $\mc D = (1,2)$ are the same as the exact one, up to a constant difference due to normalization. Outside $\mc D$, the larger $\lambda$ is associated with more rapid decline of density and therefore smaller constraint relaxation.

\begin{figure}[H]
 \centering
 \includegraphics[width=0.5\textwidth]{density_truncated_normal}
\caption{Unnormalized densities for truncated normal $\No_{(1,2)}(0,1)$, under exact and approximating densities. The exact density abruptly drops to $0$ on the two boundaries, while the approximating ones drop continuously. In the approximation, larger $\lambda$ is associated with lower tolerance for constraint relaxation ($( 1-\theta )_+$ and $( \theta - 2)_+$). All densities inside $(1,2)$ are the same up to a constant difference.}
\label{truncated_normal}
\end{figure}

Although it is temping to always induce almost $0$ relaxation with very large $\lambda$, in heavily constrained models such as the ones with equality constraint, the narrow distribution width in $\mc R$ will cause a detrimental effect in some popular algorithms such as Hamiltonian Monte Carlo. In those cases, it is rather useful to have a slightly larger relaxation, then use projection to correct the imperfection. We will illustrate this in the next two sections.

\subsection{Posterior Sampling for Extrinsic Posterior}

Extrinsic posterior is the approximation to the ones under exact formulation. As it is defined on a less restrictive space $\mc R$, it can be sampled easily. The tradiational sampling tools such as slice sampling, adaptive Metrohepolis-Hastings can be utilized. In this section, we present the sampling algorithm using Hamiltonian Monte Carlo (HMC), due to its high-level automation aided by software and excellent performance in convergence and posterior mixing. Various adaptive algorithms such as \cite{hoffman2014no} have been developed for making the new state less correlated with the current state.

In using the conventional HMC, we assume $\theta$ is $d$-dimensional and the space $\mc R$ is an Euchledean space $\mathbb R^d$ and the constraint functions $E_k(\theta)$'s and $G_k(\theta)$'s  are differentiable with respect to $\theta$. We focus on the case where $\theta$ is continuous, although discrete extension is possible \citep{zhang2012continuous}.

HMC is essentially a data augmentation based MCMC. Using a latent variable named ``veolicty'' $p\in \mathbb R^d$, the negative log-posterior function based on \eqref{extrinsic_prior} is

\begin{equation}
\begin{aligned}
H(\theta, p)& = U(\theta)+M(p),\\
\text{where } & U(\theta) = -\log\left\{ L(\theta;y)\pi_{0,\mc R}(\theta) \mc{E}(\theta) \right\},\\
& M(p) = \frac{p'\Sigma^{-1} p}{2},\end{aligned}
\end{equation}
with $\Sigma^{-1}$ a pre-specified positive definite matrix. Instead of using random walk or Gibbs sampling, HMC update $\theta$ and $p$ via Hamiltonian dynamics, satisfying differential equations:

\begin{equation}
\begin{aligned}
\label{hamiltonian}
\frac{\partial \theta (t)}{\partial t} & =\frac{\partial H(\theta, p)}{\partial p} = \Sigma^{-1}p,\\
\frac{\partial p(t)}{\partial t}& =-\frac{\partial H(\theta, p)}{\partial \theta} = -\frac{\partial U(\theta)}{\partial \theta}.
\end{aligned}
\end{equation}

At the start of each iteration, the current state of $\theta$ is viewed as $\theta(0)$ and $p(0)$ randomly generated from $\No(0, \Sigma)$. The solution to \eqref{hamiltonian} yields $\theta(t)$ and $-p(t)$ as the new state. Since Hamiltonian system is symplectic, $H(\theta(t),p(t))=H(\theta(0),p(0))$. However, in most cases, \eqref{hamiltonian} lacks closed-form solution, one has to use discrete approximation, commonly leap-frog algorithm \citep{neal2011mcmc}:


\begin{equation}
\begin{aligned}
\label{leap-frog}
p(T+ \varepsilon/2) & = p(T) - \varepsilon/2 \frac{\partial U}{\partial  \theta } ( \theta (T)),\\
 \theta (T + \varepsilon) & =  \theta (T) + \varepsilon \Sigma^{-1}p(T+  \varepsilon/2),\\
p(T+ \varepsilon) &= p(T+ \varepsilon/2) - \varepsilon/2 \frac{\partial U}{\partial  \theta } ( \theta (T + \varepsilon)),
\end{aligned}
\end{equation}
for $T=0,\varepsilon, 2\varepsilon,\ldots, (L-1)\varepsilon$, with $\varepsilon$ known as the time step, and $L$ as the total leap-frog steps within one iteration. The sequence of $\{(p(T),\theta(T))\}_T$ form a trajectory of length $L+1$ in the space of $\mathbb R^{2d}$. Since this approximating update is reversible, an Metropolis-Hastings step is taken at the end to accept $\theta(t)$ and $p(t)$ with probability 
$$1\wedge \exp  \left( - H(\theta(t),-p(t)) + H(\theta(0),p(0))\right)$$
 with $t=L\varepsilon$. Since constraints $\theta \in \mc D$ is now replaced by prior $\mc E(\theta)$, the derivative $\frac{\partial U}{\partial  \theta }$ can be computed easily and simple HMC can be directly run in space $\mc R$.

We now focus on finding the optimal time step $\varepsilon$, which can potentially impact the choice of $\lambda$. The time step $\varepsilon$ controls the stability of trajectory. When $\varepsilon$ is too large, $H$ grows exponentially with $L$, leading to very small acceptance rate. When $\varepsilon$ is too small, it leads to wasteful computation in making very local update. Therefore, it is useful to set large $\epsilon$ near a stability bound. For simple system such as $U(\theta)= {\theta^2}/{2\sigma^2}$, one can write \eqref{leap-frog} as a linear transfromation of $[\theta(T+\varepsilon),p(T+\varepsilon)]' = Q [ \theta(T),p(T)]'$ ($Q$ is a $2d\times 2d$ transition matrix), bounding the eigenvalues in $Q$ below magnitude $1$ determines the bound for $\varepsilon$. Since most systems involve nonlinear transition, analytical bound is not available, but one can empirically optimize $\varepsilon$ to be close to this bound, by tuning for acceptance rate in the Metropolis-Hastings step. Specifically, given fixed $L$, one tunes $\varepsilon$ so that the acceptance rate is close to but slightly below $1$.

For multiple-dimensional $\theta$ with $\Sigma=I$, the stability bound is roughly determined by the width of distribution in the most constrained direction  \citep{neal2011mcmc}. To provide an intuition, we focus on $L=1$. Each update in leap-frog algorithm corresponds to $\theta(\varepsilon)=\theta(0) + \varepsilon  p(0) - \varepsilon^2/2  \frac{\partial U}{\partial  \theta } ( \theta (0)) = \theta(0) + \varepsilon  p(0) + O(\varepsilon^2)$. Even with moderate $\varepsilon$, $\theta(\varepsilon)$ can be outside the support, when the support is narrow in certain direction. This is because $p(0)$ is randomly generated in all direction of $\mathbb R^d$. However, a stable trajectory should approximately preserve $U(\theta(\varepsilon))+M(p(\varepsilon)) = U(\theta(0))+M(p(0))$, since $M(p)= p'p/2 \ge 0$, $U(\theta(\varepsilon))\le  U(\theta(0))+M(p(0))$. With initial velocity $p(0)\sim N(0, I)$ and finite $U(\theta(0))$, a stable trajectory would never move to position with infinite $U(\theta(t))$, which corresponds to $0$ posterior density. Therefore, given $\theta(0)$, the stability bound on $\varepsilon$ is impacted by the smallest width of posterior support.

In extrinsic posterior, since the width of support is determined by $\lambda$ in the prior, it is important to avoid creating a support too narrow. This is especially common with strong constraints like equality, a very large $\lambda$ would force small bound on $\varepsilon$, creating inefficient bottleneck. Instead, it is rather useful to use smaller $\lambda$ to induce more relaxation, allowing discrete Hamiltonian dynamics to efficiently explore the space.

To illustrate, consider generating a random variable $\theta=(x_1,x_2)$ on a unit circle using von Mises--Fisher distribution, $\pi(\theta \mid y) \propto \exp(F'\theta)$ with $\theta'\theta =1$. This is a simple example of a random variable constraint on a $(2,1)$-Stiefel manifold $\mc D =\mc V(2,1)$. We set $F=(1,1)$ to induce a distribution widely spreaded over the manifold, generating great amount of uncertainty for assessing the sampling efficiency. We use extrinsic prior proportional to $K(\theta)= \exp(-\lambda (\theta'\theta -1)^2) \mathbbm{1}_{|\theta'\theta -1|<0.1}$. Geometrically, this prior expands the posterior support from a circle to a ring, with its width determined by $\lambda$.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.8\textwidth]{unit_circle_violation}
  \includegraphics[width=0.8\textwidth]{unit_circle_100steps}
 % \includegraphics[width=0.8\textwidth]{unit_circle_path}
 \includegraphics[width=0.8\textwidth]{unit_circle_acf}
\caption{Sampling posterior from a von Mises--Fisher distribution on a unit circle, using HMC with extrinc prior under $\lambda=10^3,10^4,10^5$. Row $1$ shows the posterior distribution of the constraint relaxation $|\theta'\theta -1|$; Row $2$ shows the path of $100$ leap-frog steps; Row $3$ shows the autocorrelation plot (ACF). Large $\lambda$ gives very small constraint relaxation, but suffers from slow mixing due to inefficient local update; smaller $\lambda$ increases the relaxation but results in excellent mixing.}
\label{unit_circle}
\end{figure}


We tested three different values of $\lambda = 10^3,10^4,10^5$. For each $\lambda$, we ran HMC for $10,000$ iterations, with $L=100$ leap-frog steps in each iteration. 
We set $\Sigma= \diag(1,1)$ in generating velocity $p$. During the initial $2,000$ iterations, the leap-frog step size $\varepsilon$ is tuned for an acceptance rate close to $0.8$, then it is fixed during the remaining part of Markov chain. The last $5,000$ iterations are used as posterior samples. Figure~\ref{unit_circle} plots the posterior distribution of constraint relaxation $|\theta'\theta -1|$, the sampling path and the autocorrelation function (ACF) for each Markov chain. Very large $\lambda=10^5$ has much less constraint relaxation; however, due to the small ring width, the Hamiltonian dynamics has to use small $\varepsilon$ and can only explore local space for each $100$ steps. This results in a very slow mixing (large autocorrelation even at 40 lags). On the other hand, smaller $\lambda=10^3$ has slightly larger constraint relaxation, but allows much more efficient exploration of the space and excellent mixing performance. We find that $\lambda=10^3$ is a good emipirical value for all the equality constraints in this paper.


\subsection{Correcting Projection to Constraint Space}
 
The Markov chain produced by HMC is geometrically ergodic under very general conditions \citep{livingstone2016geometric}. With the extrinsic posterior $\pi_K(\theta \mid y)$ as approximation to \eqref{exact_posterior}, one may be interested in further obtaining exact posterior in $\mc D$, likely for two reasons:  (i) to strictly uphold the constraints; (ii) to ease the strict relaxation control on extrisinc prior.

Letting $\theta^*$ be a random sample collected based on $\pi_K(\theta \mid y)$, there exists deterministic projection $P: \mc R\rightarrow \mc D$ and obtain $\theta^*_{\mc D}= P(\theta^*)$. Using this as proposal machineary, one can construct another Markov chain with $\pi(\theta_{\mc D}  \mid y)$ as the target distribution. Letting the current state be $\theta_{\mc D} = P(\theta)$, we generate proposal $\theta^*_{\mc D}= P(\theta^*)$ and accept it with probability:

\begin{equation}
 \begin{aligned}
 1 \wedge \frac{\pi(\theta^*_{\mc D} \mid y) \pi_K(\theta  \mid y) }{\pi(\theta_{\mc D}  \mid y)\pi_K(\theta^* \mid y)} =  1 \wedge \frac{\ L(\theta^*_{\mc D};y)\pi_{0,\mc R}(\theta^*_{\mc D})  \cdot    L(\theta;y)\pi_{0,\mc R}(\theta)   \mc E(\theta)}{\ L(\theta_{\mc D};y)\pi_{0,\mc R}(\theta_{\mc D})   \cdot L(\theta^*;y)\pi_{0,\mc R}(\theta^*)   \mc E(\theta^{*})}.
 \end{aligned}
 \end{equation}
 
The remaining task is then to optimize the projection with respect to the acceptance rate. Noting 

\begin{equation}
\begin{aligned}
% |\log( \frac{\pi(\theta^*_{\mc D} \mid y) \pi_K(\theta  \mid y) }{\pi(\theta_{\mc D}  \mid y)\pi_K(\theta^* \mid y)}) | \ge \big| |\log( \pi(\theta^*_{\mc D} \mid y)) - \log( \pi_K(\theta^* \mid y))| - |\log( \pi(\theta_{\mc D} \mid y)) - \log( \pi_K(\theta \mid y))| \big| \\
|\log( \frac{\pi(\theta^*_{\mc D} \mid y) \pi_K(\theta  \mid y) }{\pi(\theta_{\mc D}  \mid y)\pi_K(\theta^* \mid y)}) | \le |\log \left( \pi(\theta^*_{\mc D} \mid y) \right) - \log \left( \pi_K(\theta^* \mid y) \right)| + |\log \left( \pi(\theta_{\mc D} \mid y)\right) - \log\left( \pi_K(\theta \mid y)\right)|,
\end{aligned}
\end{equation}
it is sensible choose $\theta_{\mc D}\in \mc D$ to minimize the difference $Q(\theta_{\mc D})=\left|\log( \pi(\theta_{\mc D} \mid y)) - \log( \pi_K(\theta \mid y))\right|$ towards $0$ for each sample in the extrinsic posterior. Obviously, when the approximate $\theta \in \mc D$ exactly, the optimal projection would be the identity function; when $\theta \not\in \mc D$, standard constrained optimization technique can be used.

Continuing the unit circle example, we first obtained posterior sample from $\pi_K(\theta\mid y)$ with $\lambda =10^3$. The almost immediate drop to $0$ in ACF indicates a rapid convergence to the target posterior $\pi_K(\theta\mid y)$. We then obtain $\hat\theta_{\mc D} =  \underset{\theta_{\mc D}:\theta_{\mc D}'\theta_{\mc D}=1 }{\text{argmin}}|  F'\theta_{\mc D}  - \{ F'\theta  - \lambda (\theta'\theta -1)^2 \}|$ and construct the exact Markov chain. The acceptance rate is $97.9\%$.



\section{Illustration}

{\bf Example 1: Ordered Simplex}

{\bf Example 2: Monotone Spline} 

{\bf Example 3: Orthonormal Gaussian Processes} 

\section{Theory}

Posterior properiety of $\pi_{0,\mc R}(\theta) \mc E(\theta)$,  when $\pi_{0,\mc D}(\theta)$ is proper but $\pi_{0,\mc R}(\theta)$ is improper. 

\section{Application}
\section{Discussion}



\bibliography{reference}
\bibliographystyle{chicago}

\end{document}
